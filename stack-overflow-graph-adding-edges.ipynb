{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "# This uses the Answers.csv file from the 10% Stack Overflow data\n",
    "answer_file = \"Answers.csv\"\n",
    "# This edge list is the intermediate file used for graph building\n",
    "edges_list_file = \"answer_edges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "benchmark_1_data_file = \"output/benchmark_1.txt\"\n",
    "benchmark_2_data_file = \"output/benchmark_2.txt\"\n",
    "benchmark_3_data_file = \"output/benchmark_3.txt\"\n",
    "\n",
    "question_header = 'q_'\n",
    "user_header = 'u_'\n",
    "tag_header = 't_'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loads data with pands, it eats up memory, but parsing with pyspark is much more work\n",
    "df = pd.read_csv(\"Answers.csv\", encoding=\"ISO-8859-1\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question_ids and user_ids may overlap, but that does not mean questions are users!!!\n",
    "# Soln: each question_id += max_user_id\n",
    "max_user_id = df[['OwnerUserId']].max()\n",
    "max_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edge_df = df[['OwnerUserId', 'ParentId']]\n",
    "# 1. drop null values\n",
    "edge_df = edge_df.dropna()\n",
    "# 2. make parentIds unique\n",
    "edge_df = edge_df.assign(newParentId=lambda x: x.ParentId + max(max_user_id))\n",
    "edge_df = edge_df.drop(['ParentId'], axis=1)\n",
    "# 3. add weights to edges\n",
    "edge_df['EdgeWeight'] = 1\n",
    "# 4. cast the datafraem to int type\n",
    "edge_df = edge_df.astype('int32')\n",
    "edge_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by default, nx creates undirected edges, exactly what we want\n",
    "G = nx.read_edgelist(edges_list_file, nodetype=int, data=(('weight',float),))\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "all_user_ids = set()\n",
    "all_question_ids = set()\n",
    "with open(edges_list_file, 'r') as read_file:\n",
    "    for line in read_file.readlines():\n",
    "        user_id, question_id, weight = line.strip().split(' ')\n",
    "        all_user_ids.add(int(user_id))\n",
    "        all_question_ids.add(int(question_id))\n",
    "print(list(all_user_ids)[:10])\n",
    "print(list(all_question_ids)[:10])\n",
    "# should be no intersection between user_ids and question_ids\n",
    "print(len(all_user_ids.intersection(all_question_ids)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# General Data Analysis\n",
    "islands = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "print(\"connected components\", islands[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analyze how connected the graph is\n",
    "# connectivity of 0..... oh well\n",
    "from networkx.algorithms import approximation as approx\n",
    "approx.node_connectivity(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading edges from similarity score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"neighbors-10.pickle\", 'rb') as data:\n",
    "    similarity_data = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questionID = list(similarity_data.keys())\n",
    "unique_questionID = set(questionID)\n",
    "unique_newParentID = set(list(edge_df[\"newParentId\"]))\n",
    "print(\"unique_newParentID\", len(unique_newParentID))\n",
    "print(\"unique_questionID\", len(unique_questionID))\n",
    "print(\"difference:\", len(unique_questionID.difference(unique_newParentID)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark on similar edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "# parameters\n",
    "n_test_edge = 1000\n",
    "n_steps = 1000\n",
    "teleportation_alpha = 0.3\n",
    "origin_teleport_alpha = 0.45\n",
    "early_stop_threshold = 20\n",
    "\n",
    "def load_benchmark_data(benchmark_file, print_first_5_data=True):\n",
    "    benchmark_data = []\n",
    "    with open(benchmark_file, 'r') as input_file:\n",
    "        for line in input_file:\n",
    "            benchmark_data.append(line.strip().split())\n",
    "    print(np.array(benchmark_data).shape)\n",
    "    if print_first_5_data:\n",
    "        print(benchmark_data[:5])\n",
    "    return benchmark_data\n",
    "\n",
    "# returns whether x2 - y2 edge is the same as x1 - y1 edge\n",
    "# is_same_edge(1, 2, 2, 1) == is_same_edge(1, 2, 1, 2) == True\n",
    "def is_same_edge(x1, y1, x2, y2):\n",
    "    if x1 == x2:\n",
    "        return y1 == y2\n",
    "    elif x1 == y2:\n",
    "        return y1 == x2\n",
    "    return False\n",
    "\n",
    "# a random walk on the user_node (x or y)\n",
    "# pretend the direct edge (x, y) does not exist\n",
    "# returns a distribution of questions nodes\n",
    "def random_walk_on_edge(curr_model, x, y, teleportation_alpha, origin_teleport_alpha, top_n=100):\n",
    "    # curr_pos is always on user nodes\n",
    "#     starting_pos = x if x in all_user_ids else y\n",
    "    y = int(y[2:])\n",
    "    starting_pos = int(x[2:]) if x[:2] == user_header else y\n",
    "    curr_pos = starting_pos\n",
    "    reacheable_count = Counter()\n",
    "    for s in range(n_steps):\n",
    "        try:\n",
    "            potential_questions_nodes = random.sample(set(curr_model[curr_pos]), 2)\n",
    "            question_node = potential_questions_nodes[0] if not is_same_edge(x, y, potential_questions_nodes[0], curr_pos) else potential_questions_nodes[1]\n",
    "            # diff from pinterest algorithm in that we care about questions, not users\n",
    "            reacheable_count[question_node] += 1\n",
    "            if sum(reacheable_count.values()) / len(reacheable_count.values()) >= early_stop_threshold:\n",
    "                # if average > early_stop_threshold, stop\n",
    "                print('early stopping!')\n",
    "                break\n",
    "            potential_user_nodes = random.sample(set(curr_model[question_node]), 2)\n",
    "            \n",
    "            #### get similar question node from dict\n",
    "            if question_node in unique_questionID:\n",
    "                for cand in similarity_data[question_node][0]:\n",
    "                    if cand in unique_newParentID:\n",
    "                        similar_question_node = cand\n",
    "                    else:\n",
    "                        similar_question_node = question_node\n",
    "            else:\n",
    "                similar_question_node = question_node\n",
    "                \n",
    "            ##### trace back user node of the similar question from G[similar_question_node]\n",
    "            potential_similar_user_nodes = random.sample(set(curr_model[similar_question_node]), 2)\n",
    "\n",
    "            ### choice from similar questions (i.e artifical edges)\n",
    "            similar_user_node = potential_similar_user_nodes[0] if not \\\n",
    "                                    is_same_edge(x, y, potential_similar_user_nodes[0], similar_question_node) \\\n",
    "                                    else potential_similar_user_nodes[1]\n",
    "                \n",
    "            ### choice returned from random walk\n",
    "            user_node = potential_user_nodes[0] if not is_same_edge(x, y, potential_user_nodes[0], question_node) else potential_user_nodes[1]\n",
    "            if random.random() < teleportation_alpha:\n",
    "                curr_pos = starting_pos\n",
    "            elif teleportation_alpha <= random.random() < (teleportation_alpha + origin_teleport_alpha):\n",
    "                ### choice from teleporting back to origin\n",
    "                curr_pos = user_node\n",
    "            else:\n",
    "                curr_pos = similar_user_node\n",
    "                ### best threshold is teleportation_alpha = 0.3, origin_teleport_alpha = 0.45, and 0.25 will teleport to \n",
    "                ### similar questions\n",
    "        except ValueError:\n",
    "            # encouter valueError during random.sample when population is smaller than 2\n",
    "            # This only happens if we reached a deadend\n",
    "            # simply teleport back\n",
    "            curr_pos = starting_pos\n",
    "    # calculate distribution\n",
    "    tot_visits = sum(reacheable_count.values())\n",
    "    # sort visits by counts\n",
    "    all_visits = sorted(reacheable_count.items(), key=lambda x: x[1], reverse = True)\n",
    "    \n",
    "    visit_distribution = [(i[0], i[1] / tot_visits) for i in all_visits]\n",
    "    if top_n is not None:\n",
    "        visit_distribution = visit_distribution[:top_n]\n",
    "\n",
    "    return x, visit_distribution\n",
    "    \n",
    "# uses curr_model & teleportation_alpha from global variables\n",
    "def run_benchmark(curr_bench_data):\n",
    "    count = 0\n",
    "    recommendation_list = []\n",
    "    for question, user in tqdm(curr_bench_data):\n",
    "        _,curr_distr = random_walk_on_edge(curr_model, user, question, teleportation_alpha=teleportation_alpha,\\\n",
    "                                           origin_teleport_alpha = origin_teleport_alpha, top_n=100)\n",
    "        count += len([i for i in curr_distr if i[0] == question])>=1\n",
    "        recommendation_list.append((user, question, curr_distr))\n",
    "        \n",
    "    percentage = count / len(curr_bench_data)\n",
    "    print(\"Correct percentage\", percentage)\n",
    "    return percentage, recommendation_list\n",
    "\n",
    "def run_benchmark_parallel(curr_bench_data, n_core=8):\n",
    "    # split data into chucks\n",
    "    chuncks_of_data = np.array_split(curr_bench_data, n_core)\n",
    "\n",
    "    with Pool(n_core) as p:\n",
    "        final_accus = p.map(run_benchmark, chuncks_of_data)\n",
    "#     print(final_accus)\n",
    "    return final_accus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step \n",
    "1. load all user from benchmark, join with all dataset \n",
    "run random walk on all user and questions and calculate the correct percentage \n",
    "while running the random walk, store user with top 100 recommendations from random walk \n",
    "then check if targeted question is within the top 100 questions and if targeted question is within the top 100 questions of the baseline  \n",
    "\n",
    "2. Each line of benchmark is a user - question pair. the random walker \n",
    " tries to recommend top 100 questions to the user and we check to see \n",
    " if targeted question is within the top 100 questions and if targeted question is within the top 100 questions\n",
    " of the baseline \n",
    "3. we need to store user and 100 top recommendations from random walk \n",
    "and calculate the correct percentage for all users at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load benchmark file\n",
    "b1_data = load_benchmark_data(benchmark_1_data_file)\n",
    "b2_data = load_benchmark_data(benchmark_2_data_file)\n",
    "b3_data = load_benchmark_data(benchmark_3_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curr_model = G\n",
    "print(nx.info(curr_model), flush=True)\n",
    "\n",
    "for idx, curr_bench_data in enumerate([b1_data, b2_data,b3_data]):\n",
    "    accuracy, recommendation = run_benchmark_parallel(curr_bench_data)\n",
    "    save_to = \"benchmark\" + str(idx +1) + \"_add_eges.pk1\"\n",
    "    with open(save_to, 'wb') as handle:\n",
    "        pickle.dump(recommendation, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "- return top 100 randomly questions for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_user_df = edge_df.groupby([\"newParentId\"])[\"OwnerUserId\"].apply(list).reset_index(name=\"user_list\")\n",
    "question_user_df[\"num_user\"] = question_user_df[\"user_list\"].apply(lambda x: len(x))\n",
    "question_user_df[\"user\"] = question_user_df[\"user_list\"].apply(lambda x: random.sample(x,1)[0])\n",
    "question_user_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.1 64-bit ('anaconda': virtualenv)",
   "language": "python",
   "name": "python36164bitanacondavirtualenva849decffe6b4fdaa975805524ad0c31"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
